---
title: "Final Paper"
author: "Hamilton Hewlett"
format: html
editor: visual
bibliography: references.bib
---

## Introduction

Report cards, standardized testing, tax brackets, health studies, these are just a few things that can benefit and actively use quantile regression. Why categorize if you can only see the mean of the whole data set? Quantile regression allows us to see multiple mean models ranging across categories and quantiles of specific data segments. To understand how we got to benefit from these models, it is best to start a few steps before quantile regression.

Historically, people have relied on linear regression to interpret data for large data sets. Linear regression is something that is taught in grade school all the way through grad school. It is a great conceptual base and tool for measuring linearity between input and output. For the sake of simplicity, I am going to discuss simple linear regression. Ideally you can create a model based on your input to predict your output. With linear regression you can predict (look toward future values) and explain (look back on past values). Understanding what you can do with this tool creates an easier picture. Simple linear regression uses data to create a regression line to the explain your inputs relationship to the output. There are a few assumptions with linear regression that must be considered. First, your slope is constant, to have linear regression your slope cannot change with the various inputs given. The rest of the assumptions can be explained first using this model, $y=B_0+B_1 X+E$. The $B_0$ is the intercept, the $B_1X$ is the constant slope with the input of $B_1$ assigned to it. The $E$ is an error term. There are some assumptions with the error term. First, the formula is linear, so the expected value of $E$ is zero. Second, all error terms have the same variance. Third, errors are independent from one another. Fourth, errors are a normally distributed variable [@zambesi2024]. There are some things that linear regression lacks in. Linear regression does not provide a fully accurate representation of larger data sets that may have outliers. Linear regression cannot show you varying levels of data analysis. However, quantile regression can. Roger Koekner gives a great summary of what linear lacks in his presentation on quantile regression. Koekner says "Just as the mean gives an incomplete picture of a single distribution, so the regression curves gives a correspondingly incomplete picture for a set of distributions" [@koenker2010]. Roger is explaining that one single regression or one mean cannot tell the story of a data model. Roger is showing the need for quantile regression.

Quantile regression allows for you to look at certain blocks of your data. Instead of creating a mean model, you can look at sections. You can break the data down however you would like. This allows for your regression equations to have a more crisp picture of each section. The outliers in the top section of data will not influence the bottom and vice versa. The more quantiles that are added, the smaller your sample size becomes. When you break your data into these sections you are actually breaking the sample population up and only looking at what fits in the given quantile. This can potentially cause validity issues if questioned about sample size [@cookmanning2013]. You also may run into crossing which shouldn't happen but seems inevitable. It is a worry that would make you think this method should be deemed invalid. Think of 4 equations across a data set split up between the varying sections within the data. Knowing that unless a line is perfectly parallel with another, intersection is deemed to happen at some point. A non crossing model is attainable. A great advantage to quantile regression is that you gain an "understanding relationship between variables outside of the mean of the data" [@cookmanning2013]. This allows for better interpretation for non normal distributions [@cookmanning2013].

I wanted to highlight a great application example of quantile regression and the power it has to analyze large and complex data sets. Cook and Manning discuss a study that looked at raising on taxes on alcohol purchases. Participants were categorized as light, moderate, and heavy drinkers. The goal of the study was to see if price increase on alcohol would deter heavy drinkers from purchasing alcohol. If you just used linear regression, you would of been convinced that price was significant. However, when observing using quantile regression, we can see that only the moderate drinkers reduced their consumption based on price. The heavy and light drinkers were not swayed and price was not significant with them. This is an example of how powerful quantile regression can be. This method is used throughout all of our society and is a great tool to correctly interpret complex data[@cookmanning2013].

## Methods and Research Question

For the analysis being performed in this paper, we will be using non parametric quantile regression. We do not need to ensure we have met assumptions that you normally would with quantile regression. Koenker, Rey, and Aghion discuss this in their publication "Quantile Regression: 40 Years On." They discuss that nonparametric regression relaxes the strict linearity in co-variate assumptions of the foregoing methods while preserving the convenient linear in parameters structure that facilitates efficient computation" [@koenkerreyaghion2017]. They are saying that with nonparametric methods, we do not have to meet the normal linear assumptions given. We can still create quality models while not having to adhere to the norms. Assumptions like equal variances, independent errors, and normally distributed models [@zambesi2024] can be ignored. The formula for nonlinear (in parameters) quantile regression is $Q_{Y_{i}}(\tau | x) = g(x,\theta(\tau))$ [@koenkerreyaghion2017].

For my personal application of quantile regression, I want to dive into behavioral risk factor surveillance system (BRFSS) data. I want to know if the predictors of height and weight can have a significant affect on the age participants are told they have diabetes. I am going to analyze different quantiles of the data to see if certain areas of the population differ from others. My null hypothesis is going to be that height and weight are not significant predictors of age participants are told they have diabetes. If I fall below alpha of 0.05 on this collectively or for individual predictors, I will reject my null. To perform my analysis, I used R studio server version 4.4.1.

## Analysis and Results

#### Data Exploration and Results

First I would like to start with my data set. The BRFSS data set is phone survey data of participants eighteen years old or more. The Centers for Disease Control and Prevention conducts these surveys and owns the data set. There are some requirements that must be met as a state to be included in the data set, one of which being response count. Kentucky and Pennsylvania failed to meet this and were not included. For my analysis, I will only cover a few of the variables I used in the data set. I only looked at data from the state of Florida (state=12). For my predictor variables, I used DIABETE4, WEIGHT2, and HEIGHT3. DIABETE4 is a variable used for participants to share if they have ever been told they have diabetes. The acceptable responses were 1 (Yes), 2 (Yes, but female told only during pregnancy), 3 (No), 4 (No, pre-diabetes or borderline diabetes), 7 (Don't know/not sure), 9 (Refused). For the sake of this analysis, we stratified everything by 1 (Yes). WEIGHT2 is a variable for participants to share how much they weigh without shoes on. The responses were acceptable 50-0776 (Weight in pounds), 7777 (Don't know/Not sure), 9023 - 9352 (Weight in kilograms), 9999 (Refused), and BLANK (Not asked or Missing). HEIGHT3 is a variable that allowed participants to share their height without shoes on. The acceptable responses recorded that I used are imperial. The metric responses were much larger and made sense to be metric. For my response variable, I used DIABAGE4. DIABAGE4 is a variable that allows participants to share how old they were when told they first had diabetes. The acceptable responses are 1-97 (Age in years), 98 (Don't know/Not sure), 99 (Refused), and BLANK (Not asked or Missing). Below is a table with some descriptive statistics. This table is important because I want to show that I stratified everything by DIABETE4. For the sake of this analysis, it was only valid to use response (1). I showed that my data was clean and valid in omitting true NANs and the exclusive of all other responses due to my inclusion of DIABAGE4 in the below. As you can see, there are no other valid responses other than (1) that have data in them for DIABAGE4. This just further backs the validity and logical reasoning that the data set represents what it proclaims to. I also included 2 figures showing histograms of my variables DIABAGE4 and WEIGHT2.

Table 1

| Stratified by DIABETE4 |                 |                   |                  |                  |                |                |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
|                        | 1               | 2                 | 3                | 4                | 7              | 9              |
| n                      | 1952            | 78                | 9764             | 222              | 28             | 2              |
| DIABAGE4 (mean(SD))    | 54.39 (18.62)   | \-                | \-               | \-               | \-             | \-             |
| WEIGHT2 (mean(SD))     | 199.36 (50.47)  | 164.17 (36.29)    | 177.37 (45.32)   | 196.36 (51.12)   | 179.36 (55.79) | 153.00 (62.23) |
| HEIGHT3 (mean(SD))     | 610.68 (837.91) | 1115.53 (2145.01) | 709.05 (1238.72) | 738.83 (1335.65) | 508.54 (31.71) | 506.00 (1.41)  |

Figure 1

![](https://hmh56.github.io/IDS6940Hewlett/diabage.png)

Figure 2

![](https://hmh56.github.io/IDS6940Hewlett/weightage.png)

#### Modeling and Results

I would like to further explain my analysis and the steps I took to get to my models. I started out downloading an XPT file from the CDC BRFSS website. I had to rename one of the variables "\_STATE" to "state". I had to do this from the onset because R was having difficulty recognizing the underscore as a variable name when I was trying to filter later on. So, I had to make this change very early on to ensure I filtered correctly for Florida. I also filtered to see data where DIABETE4=1. This narrowed down the responses of participants who identified they had diabetes. I also put caps on the variables WEIGHT2 and HEIGHT3 as I was seeing obvious responses that could be disregarded. Below was a snippet of relevant R code I used to perform these tasks.

```{r Data Filtering}
brfssdata <- read_xpt("LLCP2023.XPT")
brfssdata <- brfssdata %>%
rename(state= '_STATE')
brfssdata$DIABETE4 <- as.factor(brfssdata$DIABETE4)

data2 <- brfssdata %>%
  filter(state == 12) %>%
  filter(DIABETE4 == 1) %>%
  filter(WEIGHT2<2500) %>%
  filter(HEIGHT3<2500) %>%
  select(DIABAGE4, WEIGHT2, HEIGHT3) %>%
  na.omit()
```

After I had my data set, I did run various descriptive statistics and visuals that I already showed earlier in this paper. I really wanted to show below the creation of my models. My names for each model stems from many model creations and also represents what quantile they are apart of. I created 3 quantiles to observe. mm25 is the 0.25 quantile, mm50 is the 0.50 quantile, and mm75 is the 0.75 quantile of my data set. That is what the tau represents in the model creation below. I used rq() from the quantreg package. I set DIABAGE4 as my response variable. Then, I used WEIGHT2 and HEIGHT3 as my predictor variables. I did explore models where I looked into the interaction between WEIGHT2 and HEIGHT3, however these models were not valid. From here, I called forward each models coefficients and then used those to create equations for each model for my graph. I named this data graph data. I added onto the existing data set I had. Finally, I created a visual. In this visual I wanted to ensure I could tell each line apart, so I made sure to color code and name them in a custom legend. I set my x axis to be WEIGHT2 and my y axis to be DIABAGE4.

```{r Model Creation}
mm25 <- rq(data2$DIABAGE4~data2$WEIGHT2 + data2$HEIGHT3, data=data2, tau = 0.25)
mm50 <- rq(data2$DIABAGE4~data2$WEIGHT2 + data2$HEIGHT3, data=data2, tau = 0.50)
mm75 <- rq(data2$DIABAGE4~data2$WEIGHT2 + data2$HEIGHT3, data=data2, tau = 0.75)

round(coefficients(mm25),5)
round(coefficients(mm50),5)
round(coefficients(mm75),5)

graphdata2 <- data2 %>%
  mutate(mm25reg = 46.12525 + WEIGHT2*-0.02405 + HEIGHT3*0.00501,
         mm50reg = 63.50396 + WEIGHT2*-0.04386 + HEIGHT3*-0.00296,
         mm75reg = 74.10256 + WEIGHT2*0.06340 + HEIGHT3*0.00466)
graphdata2 %>%
  ggplot(aes(x=WEIGHT2,y=DIABAGE4))+
  geom_point() +
  geom_line(aes(y=mm25reg, color="blue"))+
  annotate("text", x=600, y=95, label="mm25reg is blue",size=3)+
  geom_line(aes(y=mm50reg, color="green"))+
  annotate("text", x=600, y=90, label="mm50reg is green",size=3)+
  geom_line(aes(y=mm75reg, color="red"))+
  annotate("text", x=600, y=85, label="mm75reg is red",size=3)+
  labs(y="Age Informed",x="Weight")+
  theme_bw() +
  theme(legend.position = "none")
```

The graph (Figure 3) below shows that in quantiles 0.50 and 0.75, we can see an obvious trend that shows the more you weigh, the younger you are likely to be informed you have diabetes. The blue line I thought not to be trusted as the responses of at the top of the chart were not real recorded responses, they were simply just alternate options and it skewed that quantile. It was however interesting to see that this line is trending positively.

Figure 3

![](https://hmh56.github.io/IDS6940Hewlett/graph.png)

Then from here I ran the R code below to be able to interpret my final results. I created a table with my results to be able to easily interpret. However, I wanted to show how I got my results. I used the summary() function against each of the models I made. Then I realized that confint() does not get along with models made in rq(). After research online, I would of had to alter my models using a different function. I did not want to do that, so I researched how to calculate them manually in R.

```{r Results}
#summary for each model below to tell significance
summary(mm25)
summary(mm50)
summary(mm75)

#mm25 Confidence Intervals
mm25coef_estimates <- coef(mm25)
mm25se_estimates <- summary(mm25)$coefficients[,2]
mm25lower <- mm25coef_estimates - 1.96*mm25se_estimates
mm25upper <- mm25coef_estimates + 1.96*mm25se_estimates
mm25ci <- data.frame(Lower = round(mm25lower,3), Upper = round(mm25upper,3))

#mm50 Confidence Intervals
mm50coef_estimates <- coef(mm50)
mm50se_estimates <- summary(mm50)$coefficients[,2]
mm50lower <- mm50coef_estimates - 1.96*mm50se_estimates
mm50upper <- mm50coef_estimates + 1.96*mm50se_estimates
mm50ci <- data.frame(Lower = round(mm50lower,3), Upper = round(mm50upper,3))

#mm75 Confidence Intervals
mm75coef_estimates <- coef(mm75)
mm75se_estimates <- summary(mm75)$coefficients[,2]
mm75lower <- mm75coef_estimates - 1.96*mm75se_estimates
mm75upper <- mm75coef_estimates + 1.96*mm75se_estimates
mm75ci <- data.frame(Lower = round(mm75lower,3), Upper = round(mm75upper,3))
```

|        |                               | MM25     | MM50     | MM75    |
|--------|-------------------------------|----------|----------|---------|
| Height |                               |          |          |         |
|        | Coefficient                   | 0.00501  | -0.00296 | 0.00466 |
|        | Upper 95% Confidence Interval | 0.028    | 0.018    | 0.03    |
|        | Lower 95% Confidence Interval | -0.018   | -0.024   | -0.021  |
|        | P-Value                       | 0.66932  | 0.78491  | 0.71881 |
| Weight |                               |          |          |         |
|        | Coefficient                   | -0.02405 | -0.04386 | -0.0634 |
|        | Upper 95% Confidence Interval | -0.016   | -0.03    | -0.048  |
|        | Lower 95% Confidence Interval | -0.032   | -0.057   | -0.079  |
|        | P-Value                       | 0        | 0        | 0       |

## Conclusion

## References
